Identifying overfitting is crucial in machine learning to ensure that a model generalizes well to new, unseen data. Overfitting occurs when a model learns the details and noise in the training data to an extent that it negatively impacts the performance of the model on new data. Here are key indicators and methods to identify overfitting:

Performance Metrics:

Training vs. Validation Loss: If a model is overfitting, you will typically see the training loss continue to decrease while the validation loss starts to increase or stagnates after a certain point. This divergence between the training and validation loss is a classic sign of overfitting.
Training vs. Validation Accuracy: Similarly, if the training accuracy keeps improving and becomes very high, but the validation accuracy plateaus or decreases, it suggests the model is fitting too closely to the training data and not generalizing well to new data.

Visualization:
Learning Curves: Plotting the training and validation loss and accuracy over epochs can visually help spot overfitting. Any significant gap between the training and validation metrics over time is a warning sign.

Early Stopping:
This is a technique where you stop training as soon as the validation performance begins deteriorating, despite improvements in training performance. Many deep learning frameworks, including Keras, offer this as a callback function during training.

Regularization Techniques:
Techniques like L1 and L2 regularization add a penalty on the magnitude of model coefficients which can reduce overfitting by discouraging overly complex models.
Dropout is another regularization technique where randomly selected neurons are ignored during training. This helps in preventing the model from being too dependent on any single neuron and encourages a more robust feature learning.

Cross-validation:
Instead of a simple train/validation split, using cross-validation can help in assessing how well the model generalizes. In k-fold cross-validation, the training set is split into k smaller sets and the model is trained on k-1 of these sets, with the remaining part used as a mock validation set. This process is repeated k times with each of the k subsamples used exactly once as the validation data.
By monitoring these aspects, you can detect overfitting in your model. Adjusting the model's complexity, adding dropout, applying regularization, or using more data can help mitigate overfitting.